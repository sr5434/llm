# llm
An implementation of GPT-2 for TPUs. ```gpt-2-pretraining.ipynb``` is for pretraining GPT-2 Small(a predecessor to ChatGPT) on FineWeb-Edu 10b, a text corpus containing about 7.5 billion words. After 1 pass through the data, the validation loss was 7.29. Unfortunately, the model's outputs are not very comprehensible.
