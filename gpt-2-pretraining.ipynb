{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.16","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"tpu1vmV38","dataSources":[{"sourceId":8998077,"sourceType":"datasetVersion","datasetId":5420135}],"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"#!wget https://www.gutenberg.org/cache/epub/100/pg100.txt > data.txt\n!pip uninstall tensorflow -y\n!pip install tiktoken datasets tensorflow-cpu\n!pip install torch~=2.6.0 'torch_xla[tpu]~=2.6.0' -f https://storage.googleapis.com/libtpu-releases/index.html -f https://storage.googleapis.com/libtpu-wheels/index.html\n!pip install wandb -qU","metadata":{"_uuid":"5dfbedfd-ed7a-4a65-89cb-91da1451c60b","_cell_guid":"2833ea5c-fec4-4492-ac13-3f406816fe2b","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2025-03-19T21:57:30.286090Z","iopub.execute_input":"2025-03-19T21:57:30.286438Z","iopub.status.idle":"2025-03-19T22:01:01.037751Z","shell.execute_reply.started":"2025-03-19T21:57:30.286406Z","shell.execute_reply":"2025-03-19T22:01:01.036186Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Log in to your W&B account\nimport wandb\nimport random\nimport math\n\n# Use wandb-core, temporary for wandb's new backend\nwandb.require(\"core\")\nwandb.login(key=\"YOUR_KEY_HERE\")","metadata":{"_uuid":"4d36b5b7-5c8a-4e2e-8d02-88ad88a8f6d0","_cell_guid":"07f403fe-d9a3-47d0-989c-6ea4bfaf62fb","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2025-03-19T22:01:01.038600Z","iopub.execute_input":"2025-03-19T22:01:01.038867Z","iopub.status.idle":"2025-03-19T22:01:04.441596Z","shell.execute_reply.started":"2025-03-19T22:01:01.038839Z","shell.execute_reply":"2025-03-19T22:01:04.440441Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import tiktoken\nimport torch_xla as xla\n\ntokenizer = tiktoken.get_encoding(\"r50k_base\")\n\nlayers = 12\nemb_size = 768\nn_heads = 12 \ndropout = 0.1\nctx_size = 2048\nbatch_size = 16\ngrad_accumulation_steps = 16\nepochs = 1\n# ds_size = 9508395\neval_batch_size = 48\nvocab_size = tokenizer.n_vocab\nwandb.init(\n    project=\"gpt-2-pretrain\",\n    config={\n        \"epochs\": epochs,\n        \"dataset\": \"FineWeb\",\n        \"layers\": layers,\n        \"emb_size\": emb_size,\n        \"n_heads\": n_heads,\n        \"dropout\": dropout,\n        \"ctx_size\": ctx_size,\n        \"batch_size\": batch_size\n    },\n    # id=\"a92llyiu\",\n    # resume=\"must\"\n)","metadata":{"_uuid":"03da18b3-bfa6-4cb6-9534-a8ee25c0cd45","_cell_guid":"e73357e3-3a7f-40fe-ace0-a09ce158595c","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2025-03-19T22:01:04.443205Z","iopub.execute_input":"2025-03-19T22:01:04.443604Z","iopub.status.idle":"2025-03-19T22:01:34.293897Z","shell.execute_reply.started":"2025-03-19T22:01:04.443578Z","shell.execute_reply":"2025-03-19T22:01:34.292446Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch\nfrom torch import nn\nfrom torch.nn import functional as F\nimport tiktoken\nimport torch_xla as xla\nimport torch_xla.core.xla_model as xm\nimport torch_xla.distributed.parallel_loader as pl\nimport torch_xla.distributed.spmd as xs\nfrom torch_xla.experimental.spmd_fully_sharded_data_parallel import SpmdFullyShardedDataParallel as FSDPv2\nimport torch_xla.runtime as xr\nfrom torch_xla.amp import autocast, syncfree\nimport numpy as np\n\nxr.use_spmd()\n\n\nclass LayerNorm(nn.Module):\n    def __init__(self, n_dims):\n        super().__init__()\n        self.weight = nn.Parameter(torch.ones(n_dims))\n        self.bias = nn.Parameter(torch.zeros(n_dims))\n    def forward(self, x):\n        return F.layer_norm(x, self.weight.shape, self.weight, self.bias)\n\n\nclass Attention(nn.Module):\n    def __init__(self, d_model, heads, p_dropout, c_size):\n        super().__init__()\n        self.d_model = d_model\n        self.n_heads = heads\n        self.q = nn.Linear(d_model, d_model)\n        self.k = nn.Linear(d_model, d_model)\n        self.v = nn.Linear(d_model, d_model)\n        self.o = nn.Linear(d_model, d_model)\n        self.o.type = \"proj\" # Needed for initialization\n        self.dropout = nn.Dropout(p_dropout)\n        mask = torch.tril(torch.ones(c_size, c_size))\n        # Register the mask as a buffer but don't save it in state_dict to save memory\n        self.register_buffer(\"bias\", mask, persistent=False)\n    def forward(self, x):\n        B, T, C = x.size()\n        q = self.q(x).view(B, T, self.n_heads, C//self.n_heads).transpose(1, 2)\n        k = self.k(x).view(B, T, self.n_heads, C//self.n_heads).transpose(1, 2)\n        v = self.v(x).view(B, T, self.n_heads, C//self.n_heads).transpose(1, 2)\n        weights = q @ k.transpose(-2, -1)\n        weights = weights / (self.d_model**0.5)\n        mask = self.bias[:T,:T]\n        weights = weights.masked_fill(mask == 0, float(\"-inf\"))\n        weights = F.softmax(weights, dim=-1)\n        weights = self.dropout(weights)\n        weights = weights @ v\n        weights = weights.view((B, T, C))\n        weights = self.o(weights)\n        weights = self.dropout(weights)\n        return weights\n\nclass Layer(nn.Module):\n    def __init__(self, d_model, heads, p_dropout, c_size):\n        super().__init__()\n        self.layerNorm1 = LayerNorm(d_model)\n        self.attention = Attention(d_model, heads, p_dropout, c_size)\n        self.layerNorm2 = LayerNorm(d_model)\n        self.mlp = nn.Linear(d_model, 4*d_model)\n        self.mlp_proj = nn.Linear(d_model*4, d_model)\n        self.mlp_proj.type = \"proj\" # Needed for initialization\n        self.dropout = nn.Dropout(p_dropout)\n    def forward(self, x):\n        x = x + self.attention(self.layerNorm1(x))\n        l = self.mlp(self.layerNorm2(x))\n        l = F.gelu(l)\n        l = self.mlp_proj(l)\n        l = self.dropout(l)\n        x = x + l\n        return x\n\nclass Transformer(nn.Module):\n    def __init__(self, n_layers, d_model, heads, p_dropout, v_size, c_size):\n        super().__init__()\n        self.wte = nn.Embedding(v_size, d_model)\n        self.wpe = nn.Embedding(c_size, d_model)\n        # Needed because wte and wpe are initialized differently\n        self.wte.type = \"wte\"\n        self.wpe.type = \"wpe\"\n        self.layers = nn.ModuleList([Layer(d_model, heads, p_dropout, c_size) for _ in range(n_layers)])\n        self.dropout = nn.Dropout(p_dropout)\n        self.ln = LayerNorm(d_model)\n        self.lm_head = nn.Linear(d_model, v_size, bias=False)\n        self.wte.weight = self.lm_head.weight # Tying weights improves performance and reduces learnable params\n        self.apply(self._init_weights)\n    def _init_weights(self, module):\n        # Weight initialization per the GPT-2 code\n        # https://github.com/openai/gpt-2/blob/master/src/model.py\n        if isinstance(module, nn.Linear):\n            if hasattr(module, \"type\"):\n                std = 0.02/(2*len(self.layers))**0.5\n            else:\n                std = 0.02\n            nn.init.normal_(module.weight, std=std)\n            if module.bias is not None:\n                nn.init.zeros_(module.bias)\n        if isinstance(module, nn.Embedding):\n            if module.type == \"wte\":\n                nn.init.normal_(module.weight, std=0.02)\n            else:\n                #wpe is initialized with stddev of 0.01\n                nn.init.normal_(module.weight, std=0.01)\n    def forward(self, x, y=None):\n        B, T = x.shape\n        positions = torch.arange(0, T, dtype=torch.long).to(x.device)\n        token_embs = self.wte(x)\n        pos_embs = self.wpe(positions)\n        x = token_embs + pos_embs\n        x = self.dropout(x)\n        for layer in self.layers:\n            x = layer(x)\n        x = self.ln(x)\n        x = self.lm_head(x)\n        if y is not None:\n            loss = F.cross_entropy(x.view(-1, x.size(-1)), y.view(-1))\n        else:\n            loss = None\n        return x, loss\n    def generate(self, x, max_tokens):\n        for i in range(max_tokens):\n            logits, _ = self(torch.stack([x]))\n            logits = logits[:, -1, :]\n            logits = F.softmax(logits, dim=-1)\n            out = torch.multinomial(logits, 1)\n            out = out.view(-1)\n            x = torch.cat([x, out], dim=-1)\n        return x\n\ndef create_data_loader(batch_size, ctx_size, shard_len, shard_dir, num_shards, is_training=True):\n    # Create a simple dataset that generates inputs based on indices\n    class TokenDataset(torch.utils.data.Dataset):\n        def __init__(self, ctx_size, shard_len, shard_dir, num_shards):\n            self.ctx_size = ctx_size\n            self.shard_len = shard_len\n            self.shard_dir = shard_dir\n            self.num_shards = num_shards\n        def __len__(self):\n            return self.shard_len*self.num_shards - self.ctx_size\n            \n        def __getitem__(self, idx):\n            if idx % self.shard_len + self.ctx_size > self.shard_len:\n                #Spans across 2 shards\n                shard1 = idx//self.shard_len\n                shard2 = idx//self.shard_len+1\n                path1 = self.shard_dir+\"/\"+sorted(os.listdir(self.shard_dir))[shard1]\n                path2 = self.shard_dir+\"/\"+sorted(os.listdir(self.shard_dir))[shard2]\n                data_shard_1 = np.load(path1, mmap_mode=\"r\")\n                data_shard_2 = np.load(path2, mmap_mode=\"r\")\n                start_pos = idx - (shard1)*self.shard_len# Start position on shard 1\n                end_pos = self.ctx_size - (self.shard_len - start_pos)# End position on shard 2\n                x = torch.tensor(np.concatenate([data_shard_1[start_pos:], data_shard_2[:end_pos]]), dtype=torch.long)\n                y = torch.tensor(np.concatenate([data_shard_1[start_pos+1:], data_shard_2[:end_pos+1]]), dtype=torch.long)\n                del data_shard_1\n                del data_shard_2\n            else:\n                current_shard = idx//self.shard_len\n                path = self.shard_dir+\"/\"+sorted(os.listdir(self.shard_dir))[current_shard]\n                shard = np.load(path, mmap_mode=\"r\")\n                start_pos = idx - (current_shard)*self.shard_len\n                x = torch.tensor(shard[start_pos:start_pos+self.ctx_size], dtype=torch.long)\n                y = torch.tensor(shard[start_pos+1:start_pos+self.ctx_size+1], dtype=torch.long)\n                del shard\n            return x, y\n    \n    dataset = TokenDataset(ctx_size, shard_len, shard_dir, num_shards)\n    sampler = torch.utils.data.SequentialSampler(\n        dataset\n    )\n    \n    # Create the dataloader with the sampler\n    loader = torch.utils.data.DataLoader(\n        dataset,\n        batch_size=batch_size,\n        sampler=sampler,\n        drop_last=is_training,\n        num_workers=20,  # TPU has better performance with in-process data loading\n    )\n    \n    return loader, sampler\n\n\ndef train(tokenizer, layers, emb_size, n_heads, dropout, vocab_size, ctx_size, epochs, batch_size, eval_batch_size, steps, gradient_accumulation_steps):\n    device = xla.device()\n    \n    model = Transformer(layers, emb_size, n_heads, dropout, vocab_size, ctx_size).to(device)\n    lr = 6e-4\n    #SPMD Stuff\n    num_devices = xr.global_runtime_device_count()\n    mesh_shape = (num_devices, 1)\n    device_ids = np.array(range(num_devices))\n    mesh = xs.Mesh(device_ids, mesh_shape, ('fsdp', 'model'))\n    model = FSDPv2(module=model, mesh=mesh)\n    # Performance gain\n    # model = torch.compile(model, backend='openxla')\n\n    # Using GPT-3 hyperparams for betas and weight decay\n    optimizer = syncfree.AdamW(model.parameters(), lr=lr, betas=(0.9, 0.95), weight_decay=0.1)\n    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=steps*0.99, eta_min=6e-5)\n    warmup_steps = steps*0.01\n    # Set up train loaders\n    train_loader, train_sampler = create_data_loader(batch_size, ctx_size, 200000000, \"/kaggle/input/fineweb-edu-10bt-for-gpt2/train\", 49, is_training=True)\n    eval_loader, _ = create_data_loader(eval_batch_size, ctx_size, 150000000, \"/kaggle/input/fineweb-edu-10bt-for-gpt2/test\", 1, is_training=False)\n    # Wrap train loader with parallel loader for efficient TPU usage\n    train_device_loader = pl.MpDeviceLoader(train_loader, device, input_sharding=xs.ShardingSpec(mesh, ('fsdp', None)))\n    eval_device_loader = pl.MpDeviceLoader(eval_loader, device, input_sharding=xs.ShardingSpec(mesh, ('fsdp', None)))\n    # checkpoint = torch.load(\"/kaggle/input/\")\n    # model.module.load_state_dict(checkpoint['model'])\n    # optimizer.load_state_dict(checkpoint['optim'])\n    i = 0\n    for j in range(epochs):\n        train_iter = iter(train_device_loader)\n        #for step, (x, y) in enumerate(train_device_loader):\n        for step in range(steps):\n            # if i <= checkpoint['step']:\n            #     if i < warmup_steps:\n            #         lr_scale = (i + 1) / warmup_steps\n            #         for param_group in optimizer.param_groups:\n            #             param_group[\"lr\"] = lr * lr_scale\n            #     else:\n            #         scheduler.step()\n            #     continue\n            model.train()\n            train_loss = 0\n            for k in range(gradient_accumulation_steps):\n                x, y = next(train_iter)\n                # Forward pass\n                with autocast(xm.xla_device(), dtype=torch.bfloat16):\n                    _, loss = model(x, y)\n                train_loss += loss.item()\n                loss = loss/gradient_accumulation_steps\n                \n                # Backward pass and optimization\n                loss.backward()\n            train_loss /= gradient_accumulation_steps\n            torch.nn.utils.clip_grad_norm_(model.parameters(), 1)\n            xm.optimizer_step(optimizer)\n            optimizer.zero_grad()\n            # Log on master process only\n            if xm.is_master_ordinal():\n                xm.master_print(f\"Step {i}, Loss: {train_loss:.4f}, LR: {optimizer.param_groups[0]['lr']:.6f}\")\n            \n            # Learning rate scheduling\n            if i < warmup_steps:\n                lr_scale = (i + 1) / warmup_steps\n                for param_group in optimizer.param_groups:\n                    param_group[\"lr\"] = lr * lr_scale\n            else:\n                scheduler.step()\n            \n            # Evaluation and checkpointing\n            if step % 300 == 0:\n                model.eval()\n                \n                # Generate sample text (only on master process)\n                prompt = torch.tensor(tokenizer.encode(\"<|endoftext|>Once upon a time,\", allowed_special={'<|endoftext|>'})[:50], device=device)\n                with torch.no_grad():\n                    generated = model.module.generate(prompt, max_tokens=20)\n                decoded = tokenizer.decode(generated.cpu().tolist())\n                xm.master_print(f\"Sample generation:\\n{decoded}\")\n                \n                # Evaluate on validation set\n                total_loss = 0.0\n                count = 0\n                for eval_x, eval_y in eval_device_loader:\n                    with torch.no_grad():\n                        _, loss_eval = model(eval_x, eval_y)\n                    total_loss += loss_eval.item()\n                    count += 1\n                    if count == 50:\n                        break\n                # Average loss\n                avg_loss = total_loss / count if count > 0 else 0.0\n                wandb.log({\"train_loss\": train_loss, \"val_loss\": avg_loss, \"lr\": optimizer.param_groups[0]['lr'], \"sample\": decoded})\n                xm.master_print(f\"Step {i}, Test loss: {avg_loss:.4f}\")\n                # Save checkpoint\n                state_dict = {\n                    \"model\": model.module.state_dict(),\n                    \"optim\": optimizer.state_dict(),\n                    \"step\": i\n                }\n                torch.save(state_dict, f\"./ckpt-{i}.pt\")\n                xm.master_print(f\"Checkpoint saved to ./ckpt-{i}.pt\")\n            else:\n                wandb.log({\"train_loss\": train_loss, \"lr\": optimizer.param_groups[0]['lr']})\n            i+=1\n    # Save checkpoint at very end\n    state_dict = {\n        \"model\": model.module.state_dict(),\n        \"optim\": optimizer.state_dict(),\n        \"step\": i\n    }\n    torch.save(state_dict, f\"./ckpt-{i}.pt\")\n    xm.master_print(f\"FINAL Checkpoint saved to ./ckpt-{i}.pt\")","metadata":{"_uuid":"77f42ab7-b413-42de-bd07-2145123f4991","_cell_guid":"39192aba-a699-41c3-b4da-e88c20840502","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2025-03-19T22:01:34.294925Z","iopub.execute_input":"2025-03-19T22:01:34.295159Z","iopub.status.idle":"2025-03-19T22:01:34.348218Z","shell.execute_reply.started":"2025-03-19T22:01:34.295134Z","shell.execute_reply":"2025-03-19T22:01:34.347174Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\nos.environ.pop('TPU_PROCESS_ADDRESSES')","metadata":{"_uuid":"5260ccfe-a665-436f-b9f2-6bbe3584a8f0","_cell_guid":"d10548f6-a4b8-4b82-acfa-a609100030cc","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2025-03-19T22:02:50.843415Z","iopub.execute_input":"2025-03-19T22:02:50.843817Z","iopub.status.idle":"2025-03-19T22:02:50.850517Z","shell.execute_reply.started":"2025-03-19T22:02:50.843788Z","shell.execute_reply":"2025-03-19T22:02:50.849337Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"steps = epochs*((200000000*49 - ctx_size) // (batch_size*grad_accumulation_steps*ctx_size))\nprint(f\"Total steps: {steps}\")","metadata":{"_uuid":"d2df20a3-917b-45e4-a875-3df335a9e3fa","_cell_guid":"686e8135-cf9a-49e3-8102-f55bbd88bc2f","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2025-03-19T22:02:52.558968Z","iopub.execute_input":"2025-03-19T22:02:52.559399Z","iopub.status.idle":"2025-03-19T22:02:52.565283Z","shell.execute_reply.started":"2025-03-19T22:02:52.559363Z","shell.execute_reply":"2025-03-19T22:02:52.564251Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train(tokenizer, layers, emb_size, n_heads, dropout, vocab_size, ctx_size, epochs, batch_size, eval_batch_size, steps, grad_accumulation_steps)","metadata":{"_uuid":"b39d723d-61fe-4c73-8954-f4ac11d4ab20","_cell_guid":"65fc364c-9b81-4b8c-9013-ab27566fc812","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2025-03-19T22:02:53.444615Z","iopub.execute_input":"2025-03-19T22:02:53.445005Z","execution_failed":"2025-03-19T22:06:31.477Z"}},"outputs":[],"execution_count":null}]}